{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0afe63f8c2ad5156b01cb7ee1d2b9911f02fc2046f8456cd4e9e5e6fe6e9acd40",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "metadata": {
   "interpreter": {
    "hash": "afe63f8c2ad5156b01cb7ee1d2b9911f02fc2046f8456cd4e9e5e6fe6e9acd40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NLP supervised"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing the libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "## Importing the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         tweet_id   sentiment         author  \\\n0      1956967341       empty     xoshayzers   \n1      1956967666     sadness      wannamama   \n2      1956967696     sadness      coolfunky   \n3      1956967789  enthusiasm    czareaquino   \n4      1956968416     neutral      xkilljoyx   \n...           ...         ...            ...   \n39995  1753918954     neutral  showMe_Heaven   \n39996  1753919001        love       drapeaux   \n39997  1753919005        love       JenniRox   \n39998  1753919043   happiness       ipdaman1   \n39999  1753919049        love    Alpharalpha   \n\n                                                 content  \n0      @tiffanylue i know  i was listenin to bad habi...  \n1      Layin n bed with a headache  ughhhh...waitin o...  \n2                    Funeral ceremony...gloomy friday...  \n3                   wants to hang out with friends SOON!  \n4      @dannycastillo We want to trade with someone w...  \n...                                                  ...  \n39995                                   @JohnLloydTaylor  \n39996                     Happy Mothers Day  All my love  \n39997  Happy Mother's Day to all the mommies out ther...  \n39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n39999  @mopedronin bullet train from tokyo    the gf ...  \n\n[40000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"text_emotion.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "source": [
    "## Cleaning the text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\razva\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\razva\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(40000):\n",
    "  review = re.sub('@[a-zA-z0-9]*',' ',dataset['content'][i])\n",
    "  review = re.sub('https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}',' ',review)\n",
    "  if i<5:\n",
    "    print(review)\n",
    "  review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "\n",
    "  ps = PorterStemmer()\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  my_stopwords = stopwords.words('english')\n",
    "  my_stopwords.remove('not')\n",
    "  my_stopwords.remove('down')\n",
    "#   review = [ps.stem(word) for word in review if not word in set(my_stopwords)]\n",
    "  review = [lemmatizer.lemmatize(word) for word in review if not word in set(my_stopwords)]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  i know  i was listenin to bad habit earlier and i started freakin at his part =[\nLayin n bed with a headache  ughhhh...waitin on your call...\nFuneral ceremony...gloomy friday...\nwants to hang out with friends SOON!\n  We want to trade with someone who has Houston tickets, but no one will.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['know listenin bad habit earlier started freakin part', 'layin n bed headache ughhhh waitin call', 'funeral ceremony gloomy friday', 'want hang friend soon', 'want trade someone houston ticket one', 'pinging go prom bc bf like friend', 'sleep im not thinking old friend want married damn amp want scandalous', 'hmmm down', 'charlene love miss', 'sorry least friday', 'cant fall asleep', 'choked retainer', 'ugh beat stupid song get next rude', 'u watch hill london u realise tourture week week late watch itonlinelol', 'got news', 'storm electricity gone', 'agreed', 'sleepy not even late fail', 'lady gaga tweeted not impressed video leaking know', 'convinced always wanted signal give damn think lost another friend']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:20])"
   ]
  },
  {
   "source": [
    "## Building a bag of words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.87 GiB for an array with shape (40000, 26401) and data type int64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ecfa42d6a634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.87 GiB for an array with shape (40000, 26401) and data type int64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=26300)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26401\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))"
   ]
  },
  {
   "source": [
    "## Some more data preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(y[:10])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['empty' 'sadness' 'sadness' 'enthusiasm' 'neutral' 'worry' 'sadness'\n 'worry' 'sadness' 'sadness']\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Training the neuronal network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le  = LabelEncoder()\n",
    "lb = LabelBinarizer() \n",
    "\n",
    "lb.fit(y) \n",
    "lb.classes_\n",
    "y_new = lb.transform(y) \n",
    "# print(y_new)\n",
    "\n",
    "y=y_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['anger', 'boredom', 'empty', 'enthusiasm', 'fun', 'happiness',\n",
       "       'hate', 'love', 'neutral', 'relief', 'sadness', 'surprise',\n",
       "       'worry'], dtype='<U10')"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 1 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n [0 0 0 ... 1 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "source": [
    "## Splitting the dataset into the Training and Test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8000\n8000\n[[0 0 0 0 0 0 0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0 0 1 0 0 0 0]]\n[[0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "\n",
    "print(y_train[:10])\n",
    "print(y_test[:10])"
   ]
  },
  {
   "source": [
    "## Building an ANN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "source": [
    "### Initializing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "source": [
    "### Adding the first hidden layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=120, activation='relu'))"
   ]
  },
  {
   "source": [
    "### Adding the second hidden layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=120, activation='relu'))"
   ]
  },
  {
   "source": [
    "### Adding the output layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=13, activation='softmax'))"
   ]
  },
  {
   "source": [
    "## Training the ANN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Compiling the ANN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "source": [
    "## Making predictions and evaluating the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 47s 46ms/step - loss: 1.9699 - accuracy: 0.3258\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 1.4836 - accuracy: 0.4989\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.9256 - accuracy: 0.7016\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.5563 - accuracy: 0.8223\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.3682 - accuracy: 0.8819\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.2627 - accuracy: 0.9183\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 0.2029 - accuracy: 0.9373\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 0.1662 - accuracy: 0.9489\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 33s 33ms/step - loss: 0.1441 - accuracy: 0.9564\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.1267 - accuracy: 0.9625\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d6854d7940>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 10)"
   ]
  },
  {
   "source": [
    "### Predicting the test results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8000\n[[8.3911594e-08 5.8598982e-07 1.7389566e-04 1.3943470e-05 3.4837256e-05\n  3.8228061e-02 1.1456566e-03 6.6894549e-04 4.2339080e-04 1.1664007e-06\n  9.5621693e-01 1.3693666e-05 3.0786211e-03]\n [3.7932230e-10 2.5709511e-07 1.1151717e-04 4.2350866e-06 2.1302032e-08\n  4.9266532e-07 2.1369598e-07 3.2588228e-05 2.4988689e-02 3.4395481e-07\n  8.0305271e-02 3.9249012e-04 8.9416385e-01]\n [2.0480632e-06 1.2164754e-03 2.3979066e-02 3.1938504e-05 1.3862892e-05\n  5.8717886e-03 1.6684446e-04 4.8943724e-07 1.2245096e-03 5.9798581e-04\n  9.6264267e-01 1.6019627e-05 4.2362823e-03]\n [5.4626807e-12 1.2554825e-06 7.0249024e-09 4.5674287e-08 7.8438062e-11\n  6.9484471e-14 3.6961666e-07 2.3469318e-07 1.9228200e-05 8.4613989e-11\n  9.9981612e-01 1.4737009e-07 1.6270504e-04]\n [2.2297764e-09 1.3974023e-11 3.9725869e-06 2.9005061e-04 3.2505156e-03\n  9.4653291e-01 2.6386661e-07 1.7267054e-04 4.3215973e-06 1.8070044e-07\n  9.2904379e-08 4.9731404e-02 1.3672950e-05]]\n[0, 0, 7, 5, 1, 1, 1, 1, 0, 0, 0, 10, 9, 1, 7, 0, 4, 7, 0, 9, 7, 9, 7, 1, 1, 7, 5, 1, 0, 7]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "\n",
    "print(len(y_pred))\n",
    "\n",
    "y_pred_labels = []\n",
    "\n",
    "# print(y_pred[:5])\n",
    "\n",
    "for i in range(len(y_pred)): \n",
    "\n",
    "    # print(sum(y_pred[i]))\n",
    "\n",
    "    maxim=y_pred[i][0]\n",
    "    label=0\n",
    "    for j in range(1,len(y_pred[i])):\n",
    "        if maxim>y_pred[i][j]:\n",
    "            maxixm = y_pred[i][j]\n",
    "            label = j\n",
    "\n",
    "    y_pred_labels.append(label)\n",
    "\n",
    "print(y_pred_labels[:30])\n"
   ]
  },
  {
   "source": [
    "### Checking the Confusion Matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "relabeled_y_test= []\n",
    "\n",
    "for i in range(len(y_test)): \n",
    "\n",
    "    maxim=-1 \n",
    "    label=-1\n",
    "    \n",
    "\n",
    "    for j in range(len(y_test[i])):\n",
    "        if y_test[i][j] == 1: \n",
    "            relabeled_y_test.append(j)\n",
    "            break \n",
    "\n",
    "print(relabeled_y_test[:10])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[10, 8, 10, 2, 4, 5, 5, 8, 9, 5]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  3   9  47  44  73 270  53 204 588  96 308 103 462]\n [  7   6  24  39 141 435  21 343 436  81 166 130 336]\n [  0   2   1   2   4  11   5  16  17   4  18   5  26]\n [  0   0   4   2   8  23   6  15  38  10  21   7  42]\n [  0   1   7   3   0   7   9   6  32   3  44   7  57]\n [  0   7  10   4   4   5  14  11  36   6  59  17  78]\n [  1   1   7  10  27  88   3  58  92  25  45  20  79]\n [  3   3  10  11  13  41  27  16 160  22 129  37 177]\n [  1   1   0   0   4   7   7   8  19   2  20   2  31]\n [  1   4  11   9  19  25  43  30 108  11 131  34 169]\n [  3   1  12   9  38  95  13  37 121  15  32  35  96]\n [  0   1   7   8  11  23  39  10  52  12  69  18 104]\n [  0   1   7   6  12  22  11  26  44   5  20   7  37]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.019125"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_pred_labels, relabeled_y_test)\n",
    "print(cm)\n",
    "accuracy_score(y_pred_labels, relabeled_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}